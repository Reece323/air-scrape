# -*- coding: utf-8 -*-
"""wrangling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WzObNprS7_9OFBZLdJdyEifm2AsnrKjZ

<p align="center"><img src="https://github.com/Reece323/air-scrape/blob/main/assets/airbnb_logo.png?raw=true" width="300"/></p>
<center> <h1> Analyzing AirBnB Listings </h1> </center>

### This project focuses on data gathered from AirBnB listings

#### Goals:

- _Manipulating and Exploring the data_
- _Creating Visualizations_
- _Use Machine Learning_
- _Determine which features attracts consumers_
- _See how to increase revenue as `host`_

### Imports
"""

# Imports
# !pip install category_encoders rich
import json
import re
import time
import urllib.request
import warnings
warnings.filterwarnings('ignore')
from datetime import datetime

import category_encoders as ced
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

import xgboost as xgb
from xgboost import plot_importance
import lightgbm as lgb
from IPython.display import SVG

from keras import layers, models, optimizers, regularizers
from keras.utils.vis_utils import model_to_dot
from tensorflow.keras.optimizers import Adam

from matplotlib import colors as mcolors

from rich.console import Console

from sklearn.linear_model import (ElasticNet, Lasso,  BayesianRidge,
                        LassoLarsIC, LogisticRegression, LinearRegression, 
                        Ridge, Lasso)
from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor
from sklearn.kernel_ridge import KernelRidge
from sklearn.pipeline import make_pipeline, Pipeline
from sklearn.preprocessing import (RobustScaler, MinMaxScaler, StandardScaler,
                                   OneHotEncoder, RobustScaler)
from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone
from sklearn.model_selection import KFold, cross_val_score, train_test_split
from sklearn import metrics
from sklearn.compose import make_column_transformer, ColumnTransformer
from sklearn.impute import KNNImputer, SimpleImputer
from sklearn.metrics import (accuracy_score,balanced_accuracy_score, explained_variance_score,r2_score,
                             mean_squared_error, r2_score)
from sklearn.model_selection import (GridSearchCV, RandomizedSearchCV,cross_val_score,
                                     train_test_split)

from scipy import stats
from scipy.stats import norm, skew

from combine_all import df

console = Console()
print = console.print


pd.options.display.max_columns = 4000

# plt.xkcd()
sns.set_context("talk")
sns.set_context("notebook", font_scale=1, rc={"lines.linewidth": .1})

"""### Dropping unused data"""

# Austin Tx Listings
df.address = df.address.apply(lambda x: 'Austin, Texas' if 'Texas' in x else x)
df.address = df.address.apply(lambda x: 'Austin, Texas' if 'Austin' in x else x)

# NWA listings
df.address = df.address.apply(lambda x: 'Fayetteville' if 'Prairie Grove' in x else x)
df.address = df.address.apply(lambda x: 'Fayetteville' if 'Fayetteville' in x else x)
df.address = df.address.apply(lambda x: 'Fayetteville' if 'Farm' in x else x)

df.address = df.address.apply(lambda x: 'Rogers' if 'Rogers' in x else x)
df.address = df.address.apply(lambda x: 'Rogers' if 'Lowell' in x else x)


df.address = df.address.apply(lambda x: 'Bentonville' if 'Bentonville' in x else x)
df.address = df.address.apply(lambda x: 'Bentonville' if 'Centerton' in x else x)
df.address = df.address.apply(lambda x: 'Bentonville' if 'Pea Ridge' in x else x)
df.address = df.address.apply(lambda x: 'Bentonville' if 'Bentonville' in x else x)

df.address = df.address.apply(lambda x: 'Springdale' if 'Tonti' in x else x)
df.address = df.address.apply(lambda x: 'Springdale' if 'Springdale' in x else x)

df.address = df.address.apply(lambda x: 'Bella Vista' if 'Bella Vista' in x else x)
df.address = df.address.apply(lambda x: 'Bella Vista' if 'Pineville' in x else x)

# Central Ar listings
df.address = df.address.apply(lambda x: 'Hot Springs' if 'Hot Springs' in x else x)
df.address = df.address.apply(lambda x: 'Hot Springs' if 'Cave' in x else x)
df.address = df.address.apply(lambda x: 'Hot Springs' if 'Lake' in x else x)
df.address = df.address.apply(lambda x: 'Hot Springs' if 'Garland' in x else x)
df.address = df.address.apply(lambda x: 'Hot Springs' if 'Piney' in x else x)
df.address = df.address.apply(lambda x: 'Hot Springs' if 'Alex' in x else x)
df.address = df.address.apply(lambda x: 'Hot Springs' if 'Royal' in x else x)
df.address = df.address.apply(lambda x: 'Hot Springs' if 'Whittington' in x else x)
df.address = df.address.apply(lambda x: 'Hot Springs' if 'Rockwell' in x else x)
df.address = df.address.apply(lambda x: 'Hot Springs' if 'Arkansas, United States' in x else x)


df.address = df.address.apply(lambda x: 'Little Rock' if 'Little Rock' in x else x)
df.address = df.address.apply(lambda x: 'Little Rock' if 'Sherwood' in x else x)

cols_to_drop_temp = [
    'city',
    "calendar",
    # 'rootAmenitySections',
    'p3SummaryAddress',
    'primaryHost.about',
    'primaryHost.badges',
    'primaryHost.firstName',
    'primaryHost.id',
    'primaryHost.isSuperhost',
    'primaryHost.memberSinceFullStr',
    'primaryHost.name',
    'primaryHost.languages',
    'primaryHost.pictureUrl',
    'primaryHost.responseRateWithoutNa',
    'primaryHost.responseTimeWithoutNa',
    'primaryHost.hasInclusionBadge',
    'primaryHost.pictureLargeUrl',
    'primaryHost.hostIntroTags',
    'primaryHost.hostUrl',
    'primaryHost.listingsCount',
    'primaryHost.totalListingsCount',
    'tierId',
    'hasSpecialOffer',
    'hasWeWorkLocation',
    'hasWeWorkLocation',
    'additionalHosts',
    'hometourRooms',
    'hometourSections',
    'descriptionLocale',
    'initialDescriptionAuthorType',
    'localizedCheckInTimeWindow',
    'localizedCheckOutTime',
    'hometourRooms',
    'countryCode',
    'hasHostGuidebook',
    'hasLocalAttractions',
    'neighborhoodCommunityTags',
    'state',
    'paidGrowthRemarketingListingIds',
    'hasCommercialHostInfo',
    'hostSignatureFont.url',
    'nearbyAirportDistanceDescriptions',
    'propertyTypeInCity',
    'renderTierId',
    'isHotel',
    'isNewListing',
    'showReviewTag',
    'isRepresentativeInventory',
    'localizedCity',
    'highlights',
    'highlightsImpressionId',
    'pointOfInterests',
    'hostGuidebook.guidebookUrl',
    'hostGuidebook.localizedNameForHomesPdp',
    'hostGuidebook.title',
    'hostGuidebook.id',
    'pageViewType',
    'previewTags',
    'seeAllHometourSections',
    'enableHighlightsVoting',
    'heroModule.categorizedPhotos',
    'sortedReviews',
    'documentDisplayPictures',
    'sections',
    'p3ImpressionId',
    'paidGrowthRemarketingListingIdsStr',
    'isBusinessTravelReady',
    'hasHouseRules',
    'reviewsOrder',
    'hostQuote',
    'localizedListingExpectations',
    'pricing.rate.amount_formatted',
    'pricing.rate.currency',
    'pricing.rate.is_micros_accuracy',
    'pricing.rate_with_service_fee.amount_formatted',
    'pricing.rate_with_service_fee.currency',
    'pricing.rate_with_service_fee.is_micros_accuracy',
    'reviewDetailsInterface.reviewSummary',
    'reviewsModule.appreciationTags',
    'pricing.rate_with_service_fee.amount',
    'pricing.rate_type',
    'sectionedDescription.authorType',
    'sectionedDescription.locale',
    'sectionedDescription.localizedLanguageName',
    'sectionedDescription.notes',
    'sectionedDescription.space',
    # 'guestControls.allowsNonChinaUsers',
    # 'guestControls.structuredHouseRulesWithTips',
    'sectionedDescription.neighborhoodOverview',
    'sectionedDescription.access',
    'sectionedDescription.description',
    'sectionedDescription.interaction',
    'sectionedDescription.name',
    'sectionedDescription.summary',
    'sectionedDescription.transit',
    # 'name',
    'listingExpectations',
    'sectionedDescription.houseRules',
    'additionalHouseRules',
    'idStr'
]

df = df.drop(cols_to_drop_temp, axis=1)
# df.columns

"""### Cleaning numerical values in Bedrooms, Beds, and Bathrooms"""

# function to check dtypes and % of unique values
def checking(df_to_check):
    print(f"Cleaned data type: {df_to_check.dtype}")
    print(
        f'% of listings with # unique: \n{df_to_check.value_counts(normalize=True)}\n')

# 'rooms'
df['bedroomLabel'] = df['bedroomLabel'].replace('Studio', '0 bedrooms')
df['bedroomLabel'] = df['bedroomLabel'].replace('', '0 bedrooms')
new = df["bathroomLabel"].str.split(" ", n=1, expand=True)
# making separate column from new data frame
df["rooms"] = new[0]

df = df[df['url'] != 'https://www.airbnb.com/rooms/35002301']
df = df[df['url'] != 'https://www.airbnb.com/rooms/52169286']
df = df[df['url'] != 'https://www.airbnb.com/rooms/42738847']
df = df[df['bedLabel'] != '61 beds']
df = df[df['bedLabel'] != '132 beds']
df = df[df['bathroomLabel'] != '']

new1 = df["bedLabel"].str.split(" ", n=1, expand=True)
new1 = new1.replace('', '0')

# making separate column from new data frame
df["beds"] = new1[0]

new = df["bathroomLabel"].str.split(" ", n=1, expand=True)
df['baths'] = new[0].astype(float)
# checking(df.baths)
df.columns = [re.sub("[ ,\$]", "_", re.sub("[.]", "", c)) for c in df.columns]

"""### temporary dropping of data columns"""

dropme = ['name', 'country', 'listingRooms', 'roomType',
          'bedroomLabel', 'bedLabel', 'bathroomLabel',
          'seeAllAmenitySections', 'categorizedPreviewAmenities',
          'reviewsModulelocalizedOverallRating',
          'priceDetails', 'photos',
          'amenities_selectListViewPhoto', 'amenities_selectTileViewPhoto',
          'listingAmenities','amenities_description',  'reviews', 'url', 'guestControlspersonCapacity'
          ]

df = df.drop(dropme, axis=1)

# Renaming
df = df.rename(
    columns={
        'reviewDetailsInterfacereviewCount': 'reviewCount',
        'isHostedBySuperhost': 'Superhost',
        'roomTypeCategory': 'listingType',
        'pricingrateamount': 'pricepernight', 
    }
)

df.Superhost = df.Superhost.astype(np.int64)

# Reordering
df = df.reindex(
    columns=[
        'pricepernight', 'address', 'listingType', 'stars', 'reviewCount','id', 'Superhost',
        'minNights', 'maxNights','numberOfGuests', 'rooms', 'beds', 'baths', 'locationlat',
        'locationlng', 'amenities_id', 'amenities_isPresent', 'amenities_name',
        'guestControlsallowsChildren',	'guestControlsallowsEvents', 'guestControlsallowsInfants',
        'guestControlsallowsPets',	'guestControlsallowsSmoking',	
       
    ]
)

"""### Checking out current state of data versus raw data"""

# df.describe()

# df['listingType'] = df['listingType'].replace(['entire_home','private_room'],[1,0])
df.listingType.value_counts(normalize=True)
# df['city'] = df['city'].replace(['Bentonville','Bella Vista','Rogers', 'Centerton', 'Cave Springs'],[1,2,3,4,5])
df = pd.get_dummies(df, columns=['amenities_name'])
df.columns = [re.sub("[ ,\$]", "_", re.sub("[.]", "", c)) for c in df.columns]

### Checking out amenities
# print(f'DF before removing False Amenities:\n\n{df.shape}')
df = df[df.amenities_isPresent != False]
# print(f'DF AFTER removing False Amenities:\n\n{df.shape}')
df = df.drop(columns=["amenities_isPresent", "amenities_id"])
# print(f'Since all amenities are now True, removing isPresent column\n\n{df.shape}')


df['rooms'] = df['rooms'].astype(float)
df['beds'] = df['beds'].astype(float)

# When using pandas explode, all amenities had their own rows per group
# this adds them together, now having one row per all amenities per all unique ID values

id_group = df.groupby(['id'])
df = id_group.max()
df = df[df['pricepernight'].notna()]
# df

"""### Analyzing numerical data"""

for col in ['amenities_name_Air_conditioning','amenities_name_Backyard']:
  uni_val_col = df[col].unique()
  # print(f'The unique values in {col} are {uni_val_col}')

for col in ['amenities_name_Air_conditioning','amenities_name_Backyard']:
  df[col] = pd.to_numeric(df[col], errors='coerce')
  
  
## Create a dataframe with continuous columns 
df_cont = df.select_dtypes(include = ['int64','float64', 'uint8'])## Create a dataframe with categorical columns 
df_cat = df.select_dtypes(include =['object'])

imputer = KNNImputer(n_neighbors=10) 
df_data = imputer.fit_transform(df_cont)## Creating a new dataframe of the imputed data
df_num = pd.DataFrame(df_data, columns = df_cont.columns)
# df_num.sample(10).style.background_gradient(axis=0)

"""### and now categorical data"""

# Percentage of missing values in each dataframe along with visualization
total = df_cat.isnull().sum().sort_values(ascending=False)
percent = df_cat.isnull().sum()/df_cat.isnull().count().sort_values(ascending=False)
missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])
# missing_data.style.background_gradient(axis=1)

# print(f"Missing_Values_count:\n\n{df.isna().sum()}")

mean_value=df['stars'].mean()
df.stars.fillna(value=mean_value, inplace=True)

# print(f"Missing_Values_count:\n\n{df.isna().sum()}")

"""### Visualization of numerical columns"""

# ## Plot displot for continuous data columns
# for col in df_num.columns:
#   sns.histplot(df_num[col], alpha=0.5);
#   plt.figure();

"""### ...categorical columns"""

# cat_cols = ['address', 'listingType']
# ## Plotting bar plots for categorical data columns
# for col in cat_cols:
#     plt.figure()
#     sns.countplot(x=col, data=df_cat)
#     plt.xticks(rotation=90)

"""### Outliers"""

# outliers

df_outliers = pd.DataFrame(index=df_num.columns, columns=['outliers', 'outliers%'])

for col in df_num.columns:
    if any(x in str(df_num[col].dtype)for x in ['int', 'float']):
        
        df_outliers.loc[col, 'count'] = len(df_num)
        df_outliers.loc[col, 'q1'] = df_num[col].quantile(0.25)
        df_outliers.loc[col, 'q3'] = df_num[col].quantile(0.75)
        df_outliers.loc[col, 'iqr'] = df_outliers.loc[col, 'q3'] - df_outliers.loc[col, 'q1']
        df_outliers.loc[col, 'lower'] = df_outliers.loc[col, 'q1'] - (3 * df_outliers.loc[col, 'iqr'])
        df_outliers.loc[col, 'upper'] = df_outliers.loc[col, 'q3'] + (3 * df_outliers.loc[col, 'iqr'])
        df_outliers.loc[col, 'min'] = df[col].min()
        df_outliers.loc[col, 'max'] = df[col].max()
        df_outliers.loc[col, 'outliers'] = ((df_num[col] < df_outliers.loc[col, 'lower']) | (df[col] > df_outliers.loc[col,'upper'])).sum()
        df_outliers.loc[col, 'outliers%'] = np.round(df_outliers.loc[col,
        'outliers'] / len(df_num) *100)
        
# df_outliers

"""### Checking feature correlation"""

# f, ax = plt.subplots(figsize=(8, 8), edgecolor='black')
# cmap = sns.blend_palette(["#ec00ec", ".9", "#007ed9"], 9)

# sns.heatmap(df[['pricepernight',
#  'address',
#  'listingType',
#  'stars',
#  'reviewCount',
#  'Superhost',
#  'minNights',
#  'maxNights',
#  'numberOfGuests',
#  'rooms',
#  'beds',
#  'baths']].corr(), linewidth=1, cmap=cmap, center=0, square=False, cbar_kws={"shrink": .6});

"""### setting up for machine learning"""

def binary_count_and_price_plot(col, figsize=(8,3)):
    """
    Plots a simple bar chart of the counts of true and false categories in the column specified,
    next to a bar chart of the median price for each category.
    A figure size can optionally be specified.
    """
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)
    fig.suptitle(col, fontsize=16, y=1)
    plt.subplots_adjust(top=0.70) # So that the suptitle does not overlap with the ax plot titles
    
    df.groupby(col).size().plot(kind='bar', ax=ax1, color=['#ec00ec', '#007ed9'])
    ax1.set_xticklabels(labels=['false', 'true'], rotation=0)
    ax1.set_title('Category count')
    ax1.set_xlabel('')
    
    df.groupby(col).pricepernight.median().plot(kind='bar', ax=ax2, color=['#ec00ec', '#007ed9'])
    ax2.set_xticklabels(labels=['false', 'true'], rotation=0)
    ax2.set_title('Median price (£)')
    ax2.set_xlabel('')
    
    plt.show()

# for col in df.iloc[:,16:].columns:
#     binary_count_and_price_plot(col, figsize=(6,2))

# binary_count_and_price_plot('Superhost')
# print(df.Superhost.value_counts(normalize=True))

# binary_count_and_price_plot('amenities_name_Private_hot_tub')
# print(df.amenities_name_Private_hot_tub.value_counts(normalize=True))

transformed_df = pd.get_dummies(df)

def multi_collinearity_heatmap(df, figsize=(11,9)):
    
    """
    Creates a heatmap of correlations between features in the df. A figure size can optionally be set.
    """
    
    # Set the style of the visualization
    sns.set(style="white")

    # Create a covariance matrix
    corr = df.corr()

    # Generate a mask the size of our covariance matrix
    mask = np.zeros_like(corr, dtype=bool)
    mask[np.triu_indices_from(mask)] = True

    # Set up the matplotlib figure
    f, ax = plt.subplots(figsize=figsize)

    # Generate a custom diverging colormap
    cmap = sns.diverging_palette(220, 10, as_cmap=True)

    # Draw the heatmap with the mask and correct aspect ratio
    sns.heatmap(corr, mask=mask, cmap=cmap, center=0, square=True, linewidths=.5, cbar_kws={"shrink": .5}, vmax=corr[corr != 1.0].max().max());

# multi_collinearity_heatmap(transformed_df.drop(list(transformed_df.columns[transformed_df.columns.str.startswith('url')]), axis=1), figsize=(25,22))

df = pd.get_dummies(df, columns=['address','listingType'])
df.columns = [re.sub("[ ,\$]", "_", re.sub("[.]", "", c)) for c in df.columns]

target='pricepernight'
used_cols = [c for c in df.columns.tolist() if c not in [target]]
X=df[used_cols]
y=df[target]

X.columns = X.columns.str.replace('guestControlsallows', '')
X.columns = X.columns.str.replace('amenities_name_', '')
X.columns = X.columns.str.replace("Children’s", 'Children')
X.columns = X.columns.str.replace("Pack_’n_play/", 'Children')

categorical_features = X.select_dtypes(include="object").columns
numerical_features = X.select_dtypes(exclude="object").columns


# print(f'X_train:\n{X_train.shape}\n\n\ny_train:\n{y_train.shape}\n\nX_val:\n{X_val.shape}\n\n\ny_val:\n{y_val.shape}\n\n\nX_test:\n{X_test.shape}\n\n\ny_test:\n{y_test.shape}')